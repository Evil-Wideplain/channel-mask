# What Makes Good Contrastive Learning on Small-Scale Wearable-based Tasks ?
> When two positives are both generated by augmentation in 2augs

2 AUGMENTATION IN 2 VIEWS

> Therefore, convolutional neural networks are more robust than other networks for HAR

# MASK 新
## Dataset
### UCI
> --version simclr --cases random --batch_size 1024 --epochs 200 --GPU 1 --seed 888  --framework simclr --backbone TPN --criterion NTXent --aug1 resample --aug2 na --dataset UCI --data_dir .. --temperature 0.1 --weight_decay 0.0001 --p 96 --phid 96 --ratio 0.01 --lr 0.001 --opt adam --lr_scheduler const --momentum 0.9 --lr_cls 0.005 --fine_epochs 200 --val_batch_size 64 --test_batch_size 1024 --fine_opt adam --multiplier 1

train lr: 0.001 \
train lr_cls: 0.005

### USC
> --version simclr --cases random --batch_size 1024 --epochs 200 --GPU 1 --seed 888  --framework simclr --backbone TPN --criterion NTXent --aug1 resample --aug2 na --dataset USC --data_dir .. --temperature 0.1 --weight_decay 0.0001 --p 96 --phid 96 --ratio 0.01 --lr 0.001 --opt adam --lr_scheduler const --momentum 0.9 --lr_cls 0.001 --fine_epochs 200 --val_batch_size 64 --test_batch_size 1024 --fine_opt adam --multiplier 1

train lr: 0.001 \
train lr_cls: 0.001

### Motion
> '--version simclr --cases random --batch_size 1024 --epochs 200 --GPU 1 --seed 888 ' \
 '--framework simclr --backbone TPN --criterion NTXent --aug1 resample --aug2 na ' \
 '--dataset Motion --data_dir .. --temperature 0.1 --weight_decay 0.0001 --p 96 --phid 96' \
 ' --ratio 0.01 --lr 0.001 --opt adam --lr_scheduler const --momentum 0.9 --multiplier 1 ' \
 '--lr_cls 1.0 --fine_epochs 200 --val_batch_size 64 --test_batch_size 1024 ' \
 '--fine_opt adam ' \
train lr: 0.001 \
train lr_cls: 1.0







# 改进
> tf中卷积层使用的kernel_regularizers l2 对应torch中在优化器中添加weight_decay <br>
> weight_decay在torch中是对weight进行正则化操作，在tf中是对kernel 嗯 一样的 <br>
> 对于不同的数据集使用了不同的学习率 tf版本中的都是0.001 <br>
> 将数据增强的使用放到dataloader外 <br>
> 修改微调时训练数据集的batch_size为64 测试集的为2048  <br>
>

# 改进的想法
- [ ] 随机种子 我之前一直用的是88 换上 888试试
- [ ] 优化器 通过观察这几次USC的训练效果，总是在中间会出现loss上升，从而导致最小的loss出现的位置不是很靠后，也不是很小，因此可以尝试的试试衰减。Rmsprop，Adam。虽然就已经用了Adam，但是好像参数不大对，weight_decay??
- [ ] 微调和测试时候batch_size的大小

### momentum loss在初期上升据网上说是因为momentum的原因, 下面是原文 <br >
> 长期看训练loss可有效下降，但初期有个上升的过程。其原因是：带momentum的方法训练，可看作在参数值和momentum组成的二元组上，每步乘一个矩阵，然后加一个噪音。不发散，要求这个矩阵的特征值范数小于1.但是，可能有复特征值和复特征向量。一个单位实向量分解为两个复特征向量的线性组合，系数可能是大于1的。所以，虽然系数在衰减，但复的部分可能被变换到实的部分来，就出现初期loss上升的情况，直到模最大的系数被衰减到1以下。综上，初期loss上升，不一定发散，是正常的。在不带momentum的情况下，一般不应该出现这种情况。


# linear 有监督训练
- 学习率: 0.001
- 优化器: adam
- epochs: 200
- fine_epochs: 200
- 使用100%的数据训练并测试
  - UCI
    - 0.969414506262744
  - USC
    - 0.528153278963244
  - Motion
    - 0.992716303708063
- 使用10%的数据训练,使用剩余的90%数据做测试
  - UCI
    - 0.915416981335634
  - USC
    - 0.427906399354478
  - Motion
    - 0.650752125572269

# cluster
- 优化器: adam
- epochs: 200
- fine_epochs: 200
- 使用100%的数据训练并测试
  - UCI
    - lr 0.0001
    - 
  - USC
    - lr 0.001
    - 
  - Motion
    - lr 0.001
    - 
- 使用10%的数据训练,使用剩余的90%数据做测试
  - UCI
    - 
  - USC
    - 
  - Motion
    - 

# svm and cluster
- 优化器: adam
- epochs: 200
- fine_epochs: 200
- 使用100%的数据训练并测试
  - UCI
    - lr 0.0001
    - 0.9261475088269909
  - USC
    - lr 0.001
    - 0.6450739194221871
  - Motion
    - lr 0.001
    - 0.8744797859690844
- 使用10%的数据训练,使用剩余的90%数据做测试
  - UCI
    - 
  - USC
    - 
  - Motion
    - 


## 下一步计划：


# 修改记录
- 20220717
- [x] 修改投影层relu的名称，避免同一名称的影响吧
- [x] 之前训练都是使用的两层的投影层，目的是为了向Simclr对应，同时也是为了测试两层的效果，结果看起来没有达到tf的效果
  - [ ] loss的曲线图比两层的要高啊， 测试微调后的准确度
    - lr 0.01 weight_decay 0.0001 x
    - lr 0.01 weight_decay 0.001 x
    - lr 0.001 weight_decay 0.000001 x
    - lr 0.001 weight_decay 0.0 _
    - lr 0.001 weight_decay 0.0001 _
- [ ] relu drop 同名??? 
- [ ] 还有什么问题????
  - val_batch_size? 50
- 20220718
- [ ] Motion lr 0.001 学习率好像高了， 改成0.0001试下
  - weight_decay 0.000001
  - weight_decay 0.0